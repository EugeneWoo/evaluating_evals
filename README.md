<h2>Evaluating Evals for Retrieval Relevance</h2>

I created this notebook to run a quick comparison **Atla Selene** against Gemini 2.0 Flash and GPT-4o (via the open-source Phoenix Arize), by creating a straightforward comparison of their performances in classifying the relevance or irrelevance of documents retrieved by RAG. To determine relevance to the corresponding query, I evaluated their predictions against a sample of ground-truth labels (n=30) from the Wiki_QA-Train benchmark dataset.

TL;DR it should come as no surprise that **Atla Selene** won this round on account of:
- Accuracy. It was c.30% better than Gemini and GPT.
- Higher precision on 'Relevant' class (1.00 vs c.0.50). Gemini and GPT had substantially more False Positives.
- Very clean confusion matrix, with low misclassification rate (False Positive / False Negative both minimal)
- Atla and Gemini had more balanced performances between classes, as seen from the identical/very close Macro F1 and Weighted F1.
- No material differences were noted in critique quality, though a deeper dive into subjective aspects like cohesiveness, helpfulness, and faithfulness etc may reveal nuances.

Time and budget permitting, I would like to expand the breadth of metrics including thinking critically about custom metrics, and benchmark against Claude, DeepSeek, and Mistral across other use cases apart from RAG.

*BENCHMARKING RESULTS -->*

ğŸ¥‡ **Atla Selene**
	â€¢	Accuracy: 0.97 â€“ highest among all models.
	â€¢	Precision on â€œTrueâ€: 1.00 â€“ perfect, with zero false positives.
	â€¢	Recall on â€œTrueâ€: 0.94 â€“ only one false negative.
	â€¢	Balanced performance: Identical Macro F1 and Weighted F1 of 0.97.
	â€¢	Confusion Matrix: Nearly diagonal â€“ excellent separation of classes.
	â€¢	Verdict: Strongest overall, with reliable and consistent predictions.

ğŸ¥ˆ **Gemini 2.0 Flash**
	â€¢	Accuracy: 0.70 â€“ equal to GPT-4o, but with better recall on the â€œTrueâ€ class.
	â€¢	Precision / Recall:
	â€¢	â€œTrueâ€: 0.53 / 0.90 â€“ high recall, but weaker precision (many false positives).
	â€¢	â€œFalseâ€: 0.92 / 0.60 â€“ high precision, weaker recall.
	â€¢	Macro F1: 0.75 recall average; F1-score of 0.70, outperforming GPT-4o on balance.
	â€¢	Confusion Matrix: 40% of â€œFalseâ€ class mislabeled as â€œTrueâ€, similar to GPT-4o.
	â€¢	Verdict: Slightly better class balance than GPT-4o, though still not ideal.

ğŸ¥‰ **GPT-4o via Arize**
	â€¢	Accuracy: 0.67 â€“ lowest of the three.
	â€¢	Precision / Recall:
	â€¢	â€œRelevantâ€: 0.50 / 0.80
	â€¢	â€œUnrelatedâ€: 0.86 / 0.60
	â€¢	Macro F1: 0.66 â€“ dragged down by inconsistent class treatment.
	â€¢	Confusion Matrix: Shows confusion in distinguishing unrelated docs, with 40% misclassified.
	â€¢	Verdict: Acceptable in simpler use cases, but less dependable than the others.
